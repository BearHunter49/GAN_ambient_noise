{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wit61A6n5NbM",
        "colab_type": "text"
      },
      "source": [
        "# **Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egcUB1Jz46y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pylab as plt\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Conv1D, LeakyReLU, Dense, Flatten, Dropout, Reshape, UpSampling1D, ReLU, Activation\n",
        "from keras.models import load_model, Model\n",
        "from keras.optimizers import Adam, RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8O1Dv2o5aH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH = 'drive/My Drive/Colab Notebooks/GAN'\n",
        "DATA_RAW_PATH = 'data_raw/'\n",
        "DATA_PATH = 'data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vXD97eE5mLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(BASE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHAqGCU75LA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05b50c3f-0310-43ec-df6d-8290159e1cc6"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/GAN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoGTCj8x5M1V",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f_dnbXCEBy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_KIND = ['rain', 'wind', 'bonfire', 'underwater', 'river', 'forest', 'wave']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs5zKXBl5Ttr",
        "colab_type": "text"
      },
      "source": [
        "## Split ambient sound to chunk (5s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdDi62S46Gd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "31218ee0-b04a-4355-8ad8-3982d22b346d"
      },
      "source": [
        "data_list = os.listdir(DATA_RAW_PATH)\n",
        "print(data_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rain_4.mp3', 'rain_3.mp3', 'wave_1.mp3', 'rain_2.mp3', 'rain_1.mp3', 'bonfire_1.mp3', 'rain_5.mp3', 'underwater_1.mp3', 'rain_6.mp3', 'river_1.mp3', 'forest_6.mp3', 'forest_1.mp3', 'forest_2.mp3', 'wind_1.mp3', 'forest_3.mp3', 'wind_2.mp3', 'rain_7.mp3', 'wind_3.mp3', 'forest_4.mp3', 'forest_5.mp3', 'wave_3.mp3', 'wave_2.mp3', 'underwater_2.mp3', 'river_3.mp3', 'river_2.mp3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5BIZKh0A-yc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "5d5ee004-3365-4dc0-9d4e-b36fc0bc2f70"
      },
      "source": [
        "count = 0\n",
        "sound_len = 5\n",
        "max_num = 100\n",
        "\n",
        "for data in data_list:\n",
        "    print(f\"now processing {data}...\")\n",
        "    kind = data.split('_')[0]\n",
        "\n",
        "    wav, sr = librosa.load(DATA_RAW_PATH + data, sr=16000)\n",
        "    time = wav.shape[0] // sr\n",
        "    loop = time // sound_len\n",
        "\n",
        "    if loop > max_num:  \n",
        "        loop = max_num\n",
        "\n",
        "    for i in range(0, loop):\n",
        "        count += 1\n",
        "        chunk = wav[i * sr * sound_len:(i + 1) * sr * sound_len]  \n",
        "        librosa.output.write_wav(DATA_PATH + f'{kind}_{count}.wav', chunk, sr)\n",
        "    print(f\"complete {count} count!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "now processing rain_4.mp3...\n",
            "complete 51 count!\n",
            "now processing rain_3.mp3...\n",
            "complete 83 count!\n",
            "now processing wave_1.mp3...\n",
            "complete 119 count!\n",
            "now processing rain_2.mp3...\n",
            "complete 172 count!\n",
            "now processing rain_1.mp3...\n",
            "complete 224 count!\n",
            "now processing bonfire_1.mp3...\n",
            "complete 324 count!\n",
            "now processing rain_5.mp3...\n",
            "complete 424 count!\n",
            "now processing underwater_1.mp3...\n",
            "complete 524 count!\n",
            "now processing rain_6.mp3...\n",
            "complete 624 count!\n",
            "now processing river_1.mp3...\n",
            "complete 724 count!\n",
            "now processing forest_6.mp3...\n",
            "complete 824 count!\n",
            "now processing forest_1.mp3...\n",
            "complete 924 count!\n",
            "now processing forest_2.mp3...\n",
            "complete 1024 count!\n",
            "now processing wind_1.mp3...\n",
            "complete 1124 count!\n",
            "now processing forest_3.mp3...\n",
            "complete 1224 count!\n",
            "now processing wind_2.mp3...\n",
            "complete 1324 count!\n",
            "now processing rain_7.mp3...\n",
            "complete 1424 count!\n",
            "now processing wind_3.mp3...\n",
            "complete 1524 count!\n",
            "now processing forest_4.mp3...\n",
            "complete 1624 count!\n",
            "now processing forest_5.mp3...\n",
            "complete 1724 count!\n",
            "now processing wave_3.mp3...\n",
            "complete 1824 count!\n",
            "now processing wave_2.mp3...\n",
            "complete 1924 count!\n",
            "now processing underwater_2.mp3...\n",
            "complete 2024 count!\n",
            "now processing river_3.mp3...\n",
            "complete 2124 count!\n",
            "now processing river_2.mp3...\n",
            "complete 2224 count!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P93MN2V-B3SH",
        "colab_type": "text"
      },
      "source": [
        "### Get chunk from particular data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTKPQAcTCBM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wav, sr = librosa.load(DATA_RAW_PATH + 'bonfire_2.mp3', sr=16000)\n",
        "sound_len = 5\n",
        "\n",
        "time = wav.shape[0] // sr\n",
        "loop = time // sound_len\n",
        "\n",
        "count = 200\n",
        "for i in range(0, count):\n",
        "    num = random.randrange(0, loop)\n",
        "    chunk = wav[num * sr * sound_len:(num + 1) * sr * sound_len]  \n",
        "    librosa.output.write_wav(DATA_PATH + f'bonfire_part_{i}.wav', chunk, sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqn5F_s3GQFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35ad97e3-570b-4608-85ad-0d064bc821b9"
      },
      "source": [
        "# 데이터 총 갯수 확인\n",
        "data_list = glob.glob(DATA_PATH + 'wave*')\n",
        "print(len(data_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNG6MXATSisE",
        "colab_type": "text"
      },
      "source": [
        "## Integrate to array by each of kinds and save to pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHAj2phuOKk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ba7d2ab7-6d6a-4741-bc0d-b76e590a2f03"
      },
      "source": [
        "for kind in DATA_KIND:\n",
        "    data_iter = glob.glob(DATA_PATH + kind + '*')\n",
        "    print(len(data_iter))\n",
        "\n",
        "    data_array = None\n",
        "    for data in data_iter:\n",
        "        wav, sr = librosa.load(data, sr=16000)\n",
        "        wav = np.array(wav).reshape(1, -1)\n",
        "\n",
        "        if data_array is None:  # init\n",
        "            data_array = wav\n",
        "        else:\n",
        "            data_array = np.concatenate((data_array, wav), axis=0)  # (N, 80000)\n",
        "\n",
        "    print(data_array.shape)\n",
        "\n",
        "    with open(kind + '.pickle', 'wb') as f:\n",
        "        pickle.dump(data_array, f)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "(500, 80000)\n",
            "500\n",
            "(500, 80000)\n",
            "500\n",
            "(500, 80000)\n",
            "500\n",
            "(500, 80000)\n",
            "500\n",
            "(500, 80000)\n",
            "600\n",
            "(600, 80000)\n",
            "500\n",
            "(500, 80000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzRufXAF_vve",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-NjYHki_vgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dict = dict()\n",
        "for kind in DATA_KIND:\n",
        "    with open(kind + '.pickle', 'rb') as f:\n",
        "        data_dict[kind] = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMEZF89zSJjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "4426dad1-a2cf-49d3-d037-91c66afe2bf7"
      },
      "source": [
        "for key, value in data_dict.items():\n",
        "    print(key)\n",
        "    print(len(value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rain\n",
            "500\n",
            "wind\n",
            "500\n",
            "bonfire\n",
            "500\n",
            "underwater\n",
            "500\n",
            "river\n",
            "500\n",
            "forest\n",
            "600\n",
            "wave\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY0He0HeLM9o",
        "colab_type": "text"
      },
      "source": [
        "### Integrate whole data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YnmOcVeK_qb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9f5519c-dd95-4dcc-9238-d9ca92e925c6"
      },
      "source": [
        "train_data = None\n",
        "for value in data_dict.values():\n",
        "    if train_data is None:\n",
        "        train_data = value\n",
        "    else:\n",
        "        train_data = np.concatenate((train_data, value), axis=0)\n",
        "\n",
        "train_data = np.expand_dims(train_data, axis=2)\n",
        "np.random.shuffle(train_data)\n",
        "print(train_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3600, 80000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUJbyu6J1wVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c344f6a4-6d71-4a00-dee0-81091ec0269e"
      },
      "source": [
        "print(train_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3600, 80000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5LLSGMBty9",
        "colab_type": "text"
      },
      "source": [
        "### Integrate some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp32QsRFUAF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f159388-50f9-4879-bcc2-8d84cc9972b3"
      },
      "source": [
        "# 특정 sound만 학습해보기\n",
        "train_data = None\n",
        "for target in ['wind', 'wave', 'forest']:\n",
        "    if train_data is None:\n",
        "        train_data = data_dict[target]\n",
        "    else:\n",
        "        train_data = np.concatenate((train_data, data_dict[target]), axis=0)\n",
        "\n",
        "# for key, value in data_dict.items():\n",
        "#     if key in ['rain', 'forest']:\n",
        "#         if train_data is None:\n",
        "#             train_data = value\n",
        "#         else:\n",
        "#             train_data = np.concatenate((train_data, value), axis=0)\n",
        "\n",
        "train_data = np.expand_dims(train_data, axis=2)\n",
        "np.random.shuffle(train_data)\n",
        "print(train_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600, 80000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPCuZb9rz3ix",
        "colab_type": "text"
      },
      "source": [
        "# GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb9k4ZcYmnyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0IxRgzCz6XQ",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5jA_qhlz-YD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "c20ae361-31a4-434d-d85c-fed1cbd83086"
      },
      "source": [
        "# DATA = (N, 80000, 1)\n",
        "# 64 -> 1250 -> 5000 -> 20000 -> 80000\n",
        "def create_generator():\n",
        "    input = Input(shape=(latent_dim, ))\n",
        "    x = Dense(1250 * 64)(input)\n",
        "    x = ReLU()(x)\n",
        "    x = Reshape((1250, 64))(x)\n",
        "\n",
        "    x = Conv1D(128, 5, padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling1D(size=4)(x)  # 5000, 128\n",
        "    x = Conv1D(128, 4, padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling1D(size=4)(x)  # 20000, 128\n",
        "    x = Conv1D(128, 4, padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling1D(size=4)(x)  # 80000, 128\n",
        "    x = Conv1D(128, 4, padding='same')(x)\n",
        "    # x = Activation('tanh')(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv1D(256, 4, padding='same')(x)  # 80000, 256\n",
        "    # x = Activation('tanh')(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv1D(1, 4, padding='same')(x)  # 80000, 1\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "generator = create_generator()\n",
        "generator.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 64)]              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 80000)             5200000   \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 1250, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 1250, 128)         41088     \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 1250, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_3 (UpSampling1 (None, 5000, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, 5000, 128)         65664     \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 5000, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_4 (UpSampling1 (None, 20000, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 20000, 128)        65664     \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 20000, 128)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_5 (UpSampling1 (None, 80000, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 80000, 128)        65664     \n",
            "_________________________________________________________________\n",
            "re_lu_10 (ReLU)              (None, 80000, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 80000, 256)        131328    \n",
            "_________________________________________________________________\n",
            "re_lu_11 (ReLU)              (None, 80000, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_15 (Conv1D)           (None, 80000, 1)          1025      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 80000, 1)          0         \n",
            "=================================================================\n",
            "Total params: 5,570,433\n",
            "Trainable params: 5,570,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLA7ZEWez7_D",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OizHaNSKz-5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "24818ac8-e5c6-4180-8ea6-2d333de0545f"
      },
      "source": [
        "def create_discriminator():\n",
        "    input = Input(shape=(80000, 1))\n",
        "    x = Conv1D(128, 4, strides=4)(input)  # 20000, 128\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv1D(128, 4, strides=4)(x)  # 5000, 128\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv1D(128, 4, strides=4)(x)  # 1250, 128\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Conv1D(256, 4, strides=4)(x)  # 1250, 256  kernel_regularizer=keras.regularizers.l2(0.01)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dropout(0.6)(x)  # 무작위성 추가\n",
        "\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(input, x)\n",
        "\n",
        "discriminator = create_discriminator()\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 80000, 1)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 20000, 128)        640       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 20000, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 5000, 128)         65664     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 5000, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 1250, 128)         65664     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1250, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 312, 256)          131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 312, 256)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 79872)             0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 79872)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 79873     \n",
            "=================================================================\n",
            "Total params: 343,169\n",
            "Trainable params: 343,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSs-b5q9kGuZ",
        "colab_type": "text"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cklr196wkIp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In GAN, turn off train of discriminator\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = Input(shape=(latent_dim, ))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "gan_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_nzauom5kzW",
        "colab_type": "text"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15mC_lPH7w8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan.load_weights(PATH_WEIGHT + 'gan_wind_wave2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGysXQSp-kUh",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic40kyNy-goP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 300\n",
        "batch_size = 10\n",
        "PATH_RESULT = 'result_part/'\n",
        "PATH_WEIGHT = 'weight/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RLDQmMHM0oV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e421749-f7a4-4db3-cb80-837d903e1562"
      },
      "source": [
        "start = 0\n",
        "print('Training start!')\n",
        "start_time = time.time()  # Running time\n",
        "data_num = len(train_data)\n",
        "\n",
        "for step in range(0, 1 + epoch):\n",
        "\n",
        "    while True:\n",
        "        stop = start + batch_size\n",
        "        real_sounds = train_data[start:stop]\n",
        "        data_size = len(real_sounds)\n",
        "\n",
        "        random_z_vectors = np.random.normal(size=(data_size, latent_dim))\n",
        "\n",
        "        generated_sounds = generator.predict(random_z_vectors)\n",
        "        \n",
        "        fake_real_sounds = np.concatenate((generated_sounds, real_sounds), axis=0)  # (2 * batch_size, 80000)\n",
        "\n",
        "        labels = np.concatenate((np.zeros((data_size, 1)), np.ones((data_size, 1))))  # 0: 가짜, 1: 진짜\n",
        "        # labels = np.concatenate((np.full((data_size, 1), -1), np.ones((data_size, 1))))  # -1: 가짜, 1: 진짜\n",
        "        labels += 0.05 * np.random.random(labels.shape)  # 무작위성 추가\n",
        "\n",
        "        discriminator.trainable = True  # 판별자 학습 on\n",
        "        d_loss = discriminator.train_on_batch(fake_real_sounds, labels)  # discriminator train\n",
        "\n",
        "        random_z_vectors = np.random.normal(size=(data_size, latent_dim))\n",
        "\n",
        "        fake_labels = np.ones((data_size, 1))  # 가짜를 진짜처럼 만들기\n",
        "\n",
        "        discriminator.trainable = False  # 판별자 학습 off\n",
        "        g_loss = gan.train_on_batch(random_z_vectors, fake_labels)  # generator train\n",
        "\n",
        "        start += batch_size\n",
        "        if start >= data_num:\n",
        "            start = 0\n",
        "            break\n",
        "\n",
        "\n",
        "    # epoch 마다 모델 저장\n",
        "    gan.save_weights(PATH_WEIGHT + 'gan_wind_wave_forest1.h5')\n",
        "\n",
        "    print('-' * 15)\n",
        "    print('step: ', step)\n",
        "    print('d_loss: ', d_loss)\n",
        "    print('g_loss: ', g_loss)\n",
        "\n",
        "    # 50 epoch 마다 audio 저장\n",
        "    if step % 50 == 0:\n",
        "        chunk_fake = generated_sounds[0].reshape((-1))\n",
        "        chunk_fake *= 1.5  # volume up\n",
        "        librosa.output.write_wav(PATH_RESULT + f'fake_wind_wave_forest1_{step}.wav', chunk_fake, 16000)\n",
        "\n",
        "    run_time = time.time() - start_time\n",
        "    if run_time > 60:\n",
        "        minutes = run_time // 60\n",
        "        seconds = run_time % 60\n",
        "\n",
        "        print(f'Running time in step 100: {minutes}m {seconds}s')\n",
        "    else:\n",
        "        print(f'Running time in step 100: {run_time}s')\n",
        "\n",
        "    start_time = time.time()  # Running time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training start!\n",
            "---------------\n",
            "step:  0\n",
            "d_loss:  0.6717292666435242\n",
            "g_loss:  0.6885030269622803\n",
            "Running time in step 100: 8.0m 25.326019525527954s\n",
            "---------------\n",
            "step:  1\n",
            "d_loss:  0.6529699563980103\n",
            "g_loss:  0.7119371294975281\n",
            "Running time in step 100: 1.0m 54.633957862854004s\n",
            "---------------\n",
            "step:  2\n",
            "d_loss:  0.6234095692634583\n",
            "g_loss:  0.6660753488540649\n",
            "Running time in step 100: 1.0m 55.324525356292725s\n",
            "---------------\n",
            "step:  3\n",
            "d_loss:  0.7028738856315613\n",
            "g_loss:  0.6551417708396912\n",
            "Running time in step 100: 1.0m 55.80712032318115s\n",
            "---------------\n",
            "step:  4\n",
            "d_loss:  0.6854034066200256\n",
            "g_loss:  0.6624038815498352\n",
            "Running time in step 100: 1.0m 55.30296778678894s\n",
            "---------------\n",
            "step:  5\n",
            "d_loss:  0.6678317785263062\n",
            "g_loss:  0.5986063480377197\n",
            "Running time in step 100: 1.0m 54.99700355529785s\n",
            "---------------\n",
            "step:  6\n",
            "d_loss:  0.6608815789222717\n",
            "g_loss:  0.6925750374794006\n",
            "Running time in step 100: 1.0m 55.877877950668335s\n",
            "---------------\n",
            "step:  7\n",
            "d_loss:  0.6243484616279602\n",
            "g_loss:  0.709006667137146\n",
            "Running time in step 100: 1.0m 55.58633065223694s\n",
            "---------------\n",
            "step:  8\n",
            "d_loss:  0.617184042930603\n",
            "g_loss:  0.781726062297821\n",
            "Running time in step 100: 1.0m 56.459961891174316s\n",
            "---------------\n",
            "step:  9\n",
            "d_loss:  0.6213562488555908\n",
            "g_loss:  0.7248735427856445\n",
            "Running time in step 100: 2.0m 3.3381919860839844s\n",
            "---------------\n",
            "step:  10\n",
            "d_loss:  0.611890971660614\n",
            "g_loss:  0.7718019485473633\n",
            "Running time in step 100: 1.0m 56.80767512321472s\n",
            "---------------\n",
            "step:  11\n",
            "d_loss:  0.550804853439331\n",
            "g_loss:  0.9826092720031738\n",
            "Running time in step 100: 1.0m 56.42643594741821s\n",
            "---------------\n",
            "step:  12\n",
            "d_loss:  0.6073135137557983\n",
            "g_loss:  0.7579430341720581\n",
            "Running time in step 100: 1.0m 55.70598840713501s\n",
            "---------------\n",
            "step:  13\n",
            "d_loss:  0.6535875797271729\n",
            "g_loss:  0.6729394197463989\n",
            "Running time in step 100: 1.0m 56.0400173664093s\n",
            "---------------\n",
            "step:  14\n",
            "d_loss:  0.5192834734916687\n",
            "g_loss:  0.8656069040298462\n",
            "Running time in step 100: 1.0m 57.25668263435364s\n",
            "---------------\n",
            "step:  15\n",
            "d_loss:  0.7912383079528809\n",
            "g_loss:  0.6080397367477417\n",
            "Running time in step 100: 1.0m 57.631484031677246s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}